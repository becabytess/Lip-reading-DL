{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import gc\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import Any\n",
    "\n",
    "# Constants\n",
    "data_pth = '../datasets'\n",
    "videos_pth = os.path.join(data_pth, 'videos')\n",
    "aligns_pth = os.path.join(data_pth, 'alignments')\n",
    "\n",
    "# vocab setup \n",
    "vocab = 'abcdefghijklmnopqrstuvwxyz- '\n",
    "vocab_size = len(vocab)\n",
    "vti = {vocab[i]: i+1 for i in range(vocab_size)}\n",
    "vti['-'] = 0  # using 0 for ctc loss blank \n",
    "itv = {i: j for j, i in vti.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LipNetDataset(Dataset):\n",
    "    def __init__(self, videos_pth, aligns_pth):\n",
    "        self.aligns_pth = aligns_pth\n",
    "        self.videos_pth = videos_pth\n",
    "        self.speakers = os.listdir(videos_pth)\n",
    "        videos = []\n",
    "        aligns = []\n",
    "        for speaker in self.speakers:\n",
    "            video_files = os.listdir(os.path.join(self.videos_pth, speaker))\n",
    "            align_files = os.listdir(os.path.join(self.aligns_pth, speaker))\n",
    "            for video_file in video_files:\n",
    "                is_valid_file = video_file.endswith('.mpg')\n",
    "                align_file = video_file.replace('.mpg', '.align')\n",
    "                if is_valid_file and align_file in align_files:\n",
    "                    with open(os.path.join(self.aligns_pth, speaker, align_file), 'r') as f:\n",
    "                        text = f.read()\n",
    "                    if len(text) < 1:\n",
    "                        continue\n",
    "                    videos.append(os.path.join(self.videos_pth, speaker, video_file))\n",
    "                    aligns.append(os.path.join(self.aligns_pth, speaker, align_file))\n",
    "\n",
    "        self.videos = videos\n",
    "        self.aligns = aligns\n",
    "\n",
    "    def extract_text(self, align):\n",
    "        with open(align, 'r') as f:\n",
    "            text = f.read()\n",
    "        text = ''.join(char for char in text if char in vocab)\n",
    "        return [vti[char] for char in text]\n",
    "    \n",
    "    def extract_frames(self, video):\n",
    "        frames = []\n",
    "        cap = cv2.VideoCapture(video)\n",
    "\n",
    "        while True:\n",
    "            res, frame = cap.read()\n",
    "            if not res:\n",
    "                break\n",
    "          \n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) / 255.0 \n",
    "            frames.append(torch.tensor(frame, dtype=torch.float32))\n",
    "            \n",
    "        cap.release()\n",
    "        return torch.stack(frames).float()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.videos)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        frames = self.extract_frames(video=self.videos[index])\n",
    "        align = self.extract_text(self.aligns[index]) \n",
    "        align = torch.tensor(data=align, dtype=torch.long)\n",
    "        return frames, align\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def collate_fn(batch):\n",
    "    videos = []\n",
    "    labels = []\n",
    "    for video, label in batch:\n",
    "        if len(video) == 75:  # because the dataset contains some videos with less than 75 frames\n",
    "            videos.append(video)\n",
    "            labels.append(label)\n",
    "    \n",
    "    if not videos:  \n",
    "        return None, None\n",
    "        \n",
    "    return torch.stack(videos), list(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, in_channels, frames, height, width, hidden_size=100):\n",
    "        super().__init__()\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.frames = frames\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv3d(in_channels=in_channels, out_channels=32, kernel_size=(3,3,3), \n",
    "                     stride=(1,1,1), padding=(1,1,1)),\n",
    "            nn.BatchNorm3d(num_features=32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=(1,2,2), stride=(1,2,2), padding=(0,0,0)),\n",
    "            \n",
    "            nn.Conv3d(in_channels=32, out_channels=16, kernel_size=(3,3,3), \n",
    "                     stride=(1,1,1), padding=(1,1,1)),\n",
    "            nn.BatchNorm3d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=(1,2,2), stride=(1,2,2), padding=(0,0,0)),\n",
    "            \n",
    "            nn.Conv3d(in_channels=16, out_channels=32, kernel_size=(3,3,3), \n",
    "                     stride=(1,1,1), padding=(1,1,1)),\n",
    "            nn.BatchNorm3d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=(1,2,2), stride=(1,2,2), padding=(0,0,0))\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.cnn_output_size = (width//8) * (height//8) * 32\n",
    "        \n",
    "       \n",
    "        self.forget = nn.Sequential(\n",
    "            nn.Linear(in_features=self.cnn_output_size + hidden_size, out_features=hidden_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        \n",
    "        self.candidate = nn.Sequential(\n",
    "            nn.Linear(in_features=hidden_size + self.cnn_output_size, out_features=hidden_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.input = nn.Sequential(\n",
    "            nn.Linear(in_features=hidden_size + self.cnn_output_size, out_features=hidden_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "       \n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(hidden_size + self.cnn_output_size, hidden_size),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(2 * hidden_size, vocab_size)\n",
    "        \n",
    "      \n",
    "\n",
    "    def bidirectional(self, X, isbackward=False):\n",
    "        \n",
    "        conv_out = self.conv(X)\n",
    "        b, c, f, h, w = conv_out.size()\n",
    "        conv_out = conv_out.contiguous().view(b, f, -1)\n",
    "        \n",
    "        \n",
    "        cell_state = torch.zeros(b, self.hidden_size, device=X.device)\n",
    "        hidden_state = torch.zeros(b, f, self.hidden_size, device=X.device)\n",
    "       \n",
    "        frame_range = range(f-1, -1, -1) if isbackward else range(f)\n",
    "        \n",
    "        for t in frame_range:\n",
    "            xt = conv_out[:, t, :]\n",
    "            prev_idx = t+1 if isbackward else t-1\n",
    "            valid_prev = (prev_idx >= 0 and prev_idx < f)\n",
    "            prev_hs = hidden_state[:, prev_idx, :] if valid_prev else torch.zeros(b, self.hidden_size, device=X.device)\n",
    "            \n",
    "            \n",
    "            combined = torch.cat([xt, prev_hs], dim=1)\n",
    "            \n",
    "\n",
    "            forget_gate = self.forget(combined)\n",
    "            input_gate = self.input(combined)\n",
    "            candidate_gate = self.candidate(combined)\n",
    "            output_gate = self.output(combined)\n",
    "            \n",
    "            \n",
    "            new_cell_state = forget_gate * cell_state + input_gate * candidate_gate\n",
    "            new_cell_state = torch.clamp(new_cell_state, -10, 10)  \n",
    "            new_hidden_state = output_gate * torch.tanh(new_cell_state)\n",
    "            \n",
    "            cell_state = new_cell_state\n",
    "            hidden_state[:, t, :] = new_hidden_state\n",
    "            \n",
    "        return hidden_state\n",
    "\n",
    "    def forward(self, X):\n",
    "        \n",
    "        forward_hidden = self.bidirectional(X)\n",
    "        backward_hidden = self.bidirectional(X, isbackward=True)\n",
    "        hidden_state = torch.cat([forward_hidden, backward_hidden], dim=2)\n",
    "        logits = self.classifier(hidden_state)\n",
    "        return nn.functional.log_softmax(logits, dim=2)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def clear_gpu_cache():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "dataset = LipNetDataset(videos_pth, aligns_pth)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "epochs = 300\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device:{device}\")\n",
    "\n",
    "\n",
    "# Get sample input dimensions for model initialization\n",
    "sample_batch = next(iter(dataloader))\n",
    "b, f, h, w, c = sample_batch[0].shape\n",
    "model = Model(c, f, h, w).to(device)\n",
    "ctc_loss = nn.CTCLoss(blank=0, zero_infinity=True, reduction='mean')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)  \n",
    "grad_accumulation_steps = 4\n",
    "    \n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20812708"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "n_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1/300\n",
      "Epoch 1, Batch 5, Loss: 3.09963\n",
      "Epoch 1, Batch 10, Loss: 2.76427\n",
      "Epoch 1, Batch 15, Loss: 2.59446\n",
      "Epoch 1, Batch 20, Loss: 2.42905\n",
      "Epoch 1, Batch 25, Loss: 2.30880\n",
      "Epoch 1, Batch 30, Loss: 2.14108\n",
      "Epoch 1, Batch 35, Loss: 2.09331\n",
      "Epoch 1, Batch 40, Loss: 2.21319\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "        model.train()\n",
    "        print(f\"Starting epoch {epoch+1}/{epochs}\")\n",
    "        batch_count = 0\n",
    "        epoch_loss = 0\n",
    "        valid_batches = 0\n",
    "        optimizer.zero_grad()   \n",
    "        for data, labels in dataloader:\n",
    "            if data is None or len(data) == 0:\n",
    "                continue\n",
    "                \n",
    "            batch_count += 1\n",
    "            \n",
    "            try:\n",
    "                \n",
    "                data = data.permute(0, 4, 1, 2, 3).contiguous().to(device)\n",
    "                \n",
    "                #for ctc loss\n",
    "                target_lengths = torch.tensor([len(yi) for yi in labels], device=device)\n",
    "                input_lengths = torch.full((len(labels),), 75, device=device)\n",
    "                \n",
    "                # stack labels for ctc loss\n",
    "                stacked_labels = torch.cat([label.to(device) for label in labels])\n",
    "                \n",
    "               \n",
    "                log_probs = model(data)  \n",
    "                log_probs = log_probs.permute(1, 0, 2)  \n",
    "                    \n",
    "                   \n",
    "                loss = ctc_loss(log_probs, stacked_labels, input_lengths, target_lengths)\n",
    "                    \n",
    "                    \n",
    "                loss = loss / grad_accumulation_steps \n",
    "                \n",
    "                \n",
    "                if torch.isnan(loss) or torch.isinf(loss):\n",
    "                    print(f\"Batch {batch_count}: nan or inf loss detected, skipping\")\n",
    "                    # Clear any gradients from this batch\n",
    "                    optimizer.zero_grad()\n",
    "                    continue\n",
    "                \n",
    "                \n",
    "                loss.backward()\n",
    "                \n",
    "                \n",
    "                if batch_count % grad_accumulation_steps == 0:\n",
    "                \n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5) #to avoid exploding grads ( i was getting a lot of NaN and this helped)\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    clear_gpu_cache()\n",
    "                \n",
    "\n",
    "                epoch_loss += loss.item() * grad_accumulation_steps\n",
    "                valid_batches += 1\n",
    "\n",
    "                \n",
    "                if batch_count % 5 == 0:\n",
    "                    print(f\"Epoch {epoch+1}, Batch {batch_count}, Loss: {loss.item() * grad_accumulation_steps:.5f}\")\n",
    "                \n",
    "                \n",
    "                del data, log_probs, loss\n",
    "                clear_gpu_cache()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in batch {batch_count}: {e}\")\n",
    "                clear_gpu_cache()\n",
    "             \n",
    "        if valid_batches > 0:\n",
    "           avg_loss  = epoch_loss/valid_batches\n",
    "           scheduler.step(avg_loss)\n",
    "           print(f\"Epoch {epoch+1} complete. Average loss: {avg_loss:.4f}, Valid batches: {valid_batches}/{batch_count}\")\n",
    "            \n",
    "            # Save model checkpoint\n",
    "           torch.save({\n",
    "            'model_state_dict':model.state.dict(),\n",
    "            'optimizer_state_dict':optimizer.state_dict(),\n",
    "            'loss':avg_loss,\n",
    "            \n",
    "\n",
    "           },f'model_checkpoint_epoch_{epoch+1}.pt')\n",
    "           clear_gpu_cache()\n",
    "        else:\n",
    "            print(f\"Epoch {epoch+1} had no valid batches\")\n",
    "else:\n",
    "    print(\"No valid data found\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
