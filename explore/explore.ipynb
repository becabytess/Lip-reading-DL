{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import gc\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import Any\n",
    "\n",
    "# Constants\n",
    "data_pth = '../datasets'\n",
    "videos_pth = os.path.join(data_pth, 'videos')\n",
    "aligns_pth = os.path.join(data_pth, 'alignments')\n",
    "\n",
    "# vocab setup \n",
    "vocab = 'abcdefghijklmnopqrstuvwxyz- '\n",
    "vocab_size = len(vocab)\n",
    "vti = {vocab[i]: i+1 for i in range(vocab_size)}\n",
    "vti['-'] = 0  # using 0 for ctc loss blank \n",
    "itv = {i: j for j, i in vti.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LipNetDataset(Dataset):\n",
    "    def __init__(self, videos_pth, aligns_pth):\n",
    "        self.aligns_pth = aligns_pth\n",
    "        self.videos_pth = videos_pth\n",
    "        self.speakers = os.listdir(videos_pth)\n",
    "        videos = []\n",
    "        aligns = []\n",
    "        for speaker in self.speakers:\n",
    "            video_files = os.listdir(os.path.join(self.videos_pth, speaker))\n",
    "            align_files = os.listdir(os.path.join(self.aligns_pth, speaker))\n",
    "            for video_file in video_files:\n",
    "                is_valid_file = video_file.endswith('.mpg')\n",
    "                align_file = video_file.replace('.mpg', '.align')\n",
    "                if is_valid_file and align_file in align_files:\n",
    "                    with open(os.path.join(self.aligns_pth, speaker, align_file), 'r') as f:\n",
    "                        text = f.read()\n",
    "                    if len(text) < 1:\n",
    "                        continue\n",
    "                    videos.append(os.path.join(self.videos_pth, speaker, video_file))\n",
    "                    aligns.append(os.path.join(self.aligns_pth, speaker, align_file))\n",
    "\n",
    "        self.videos = videos\n",
    "        self.aligns = aligns\n",
    "\n",
    "    def extract_text(self, align):\n",
    "        with open(align, 'r') as f:\n",
    "            text = f.read()\n",
    "        text = ''.join(char for char in text if char in vocab)\n",
    "        return [vti[char] for char in text]\n",
    "    \n",
    "    def extract_frames(self, video):\n",
    "        frames = []\n",
    "        cap = cv2.VideoCapture(video)\n",
    "\n",
    "        while True:\n",
    "            res, frame = cap.read()\n",
    "            if not res:\n",
    "                break\n",
    "          \n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) / 255.0 \n",
    "            frames.append(torch.tensor(frame, dtype=torch.float32))\n",
    "            \n",
    "        cap.release()\n",
    "        return torch.stack(frames).float()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.videos)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        frames = self.extract_frames(video=self.videos[index])\n",
    "        align = self.extract_text(self.aligns[index]) \n",
    "        align = torch.tensor(data=align, dtype=torch.long)\n",
    "        return frames, align\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def collate_fn(batch):\n",
    "    videos = []\n",
    "    labels = []\n",
    "    for video, label in batch:\n",
    "        if len(video) == 75:  # because the dataset contains some videos with less than 75 frames\n",
    "            videos.append(video)\n",
    "            labels.append(label)\n",
    "    \n",
    "    if not videos:  \n",
    "        return None, None\n",
    "        \n",
    "    return torch.stack(videos), list(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, in_channels, frames, height, width, hidden_size=100):\n",
    "        super().__init__()\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.frames = frames\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv3d(in_channels=in_channels, out_channels=32, kernel_size=(3,3,3), \n",
    "                     stride=(1,1,1), padding=(1,1,1)),\n",
    "            nn.BatchNorm3d(num_features=32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=(1,2,2), stride=(1,2,2), padding=(0,0,0)),\n",
    "            \n",
    "            nn.Conv3d(in_channels=32, out_channels=16, kernel_size=(3,3,3), \n",
    "                     stride=(1,1,1), padding=(1,1,1)),\n",
    "            nn.BatchNorm3d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=(1,2,2), stride=(1,2,2), padding=(0,0,0)),\n",
    "            \n",
    "            nn.Conv3d(in_channels=16, out_channels=32, kernel_size=(3,3,3), \n",
    "                     stride=(1,1,1), padding=(1,1,1)),\n",
    "            nn.BatchNorm3d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool3d(kernel_size=(1,2,2), stride=(1,2,2), padding=(0,0,0))\n",
    "        )\n",
    "        \n",
    "        \n",
    "        self.cnn_output_size = (width//8) * (height//8) * 32\n",
    "        \n",
    "       \n",
    "        self.forget = nn.Sequential(\n",
    "            nn.Linear(in_features=self.cnn_output_size + hidden_size, out_features=hidden_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        \n",
    "        self.candidate = nn.Sequential(\n",
    "            nn.Linear(in_features=hidden_size + self.cnn_output_size, out_features=hidden_size),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        \n",
    "        self.input = nn.Sequential(\n",
    "            nn.Linear(in_features=hidden_size + self.cnn_output_size, out_features=hidden_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "       \n",
    "        self.output = nn.Sequential(\n",
    "            nn.Linear(hidden_size + self.cnn_output_size, hidden_size),\n",
    "            nn.LayerNorm(hidden_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Linear(2 * hidden_size, vocab_size)\n",
    "        \n",
    "      \n",
    "\n",
    "    def bidirectional(self, X, isbackward=False):\n",
    "        \n",
    "        conv_out = self.conv(X)\n",
    "        b, c, f, h, w = conv_out.size()\n",
    "        conv_out = conv_out.contiguous().view(b, f, -1)\n",
    "        \n",
    "        \n",
    "        cell_state = torch.zeros(b, self.hidden_size, device=X.device)\n",
    "        hidden_state = torch.zeros(b, f, self.hidden_size, device=X.device)\n",
    "       \n",
    "        frame_range = range(f-1, -1, -1) if isbackward else range(f)\n",
    "        \n",
    "        for t in frame_range:\n",
    "            xt = conv_out[:, t, :]\n",
    "            prev_idx = t+1 if isbackward else t-1\n",
    "            valid_prev = (prev_idx >= 0 and prev_idx < f)\n",
    "            prev_hs = hidden_state[:, prev_idx, :] if valid_prev else torch.zeros(b, self.hidden_size, device=X.device)\n",
    "            \n",
    "            \n",
    "            combined = torch.cat([xt, prev_hs], dim=1)\n",
    "            \n",
    "\n",
    "            forget_gate = self.forget(combined)\n",
    "            input_gate = self.input(combined)\n",
    "            candidate_gate = self.candidate(combined)\n",
    "            output_gate = self.output(combined)\n",
    "            \n",
    "            \n",
    "            new_cell_state = forget_gate * cell_state + input_gate * candidate_gate\n",
    "            new_cell_state = torch.clamp(new_cell_state, -10, 10)  \n",
    "            new_hidden_state = output_gate * torch.tanh(new_cell_state)\n",
    "            \n",
    "            cell_state = new_cell_state\n",
    "            hidden_state[:, t, :] = new_hidden_state\n",
    "            \n",
    "        return hidden_state\n",
    "\n",
    "    def forward(self, X):\n",
    "        \n",
    "        forward_hidden = self.bidirectional(X)\n",
    "        backward_hidden = self.bidirectional(X, isbackward=True)\n",
    "        hidden_state = torch.cat([forward_hidden, backward_hidden], dim=2)\n",
    "        logits = self.classifier(hidden_state)\n",
    "        return nn.functional.log_softmax(logits, dim=2)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device:cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def clear_gpu_cache():\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "dataset = LipNetDataset(videos_pth, aligns_pth)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)\n",
    "epochs = 300\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device:{device}\")\n",
    "\n",
    "\n",
    "# Get sample input dimensions for model initialization\n",
    "sample_batch = next(iter(dataloader))\n",
    "b, f, h, w, c = sample_batch[0].shape\n",
    "model = Model(c, f, h, w).to(device)\n",
    "ctc_loss = nn.CTCLoss(blank=0, zero_infinity=True, reduction='mean')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5, verbose=True)  \n",
    "grad_accumulation_steps = 4\n",
    "    \n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20812708"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_params = sum(p.numel() for p in model.parameters())\n",
    "n_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1/300\n",
      "Epoch 1, Batch 5, Loss: 3.09963\n",
      "Epoch 1, Batch 10, Loss: 2.76427\n",
      "Epoch 1, Batch 15, Loss: 2.59446\n",
      "Epoch 1, Batch 20, Loss: 2.42905\n",
      "Epoch 1, Batch 25, Loss: 2.30880\n",
      "Epoch 1, Batch 30, Loss: 2.14108\n",
      "Epoch 1, Batch 35, Loss: 2.09331\n",
      "Epoch 1, Batch 40, Loss: 2.21319\n",
      "Epoch 1, Batch 45, Loss: 2.09999\n",
      "Epoch 1, Batch 50, Loss: 2.12297\n",
      "Epoch 1, Batch 55, Loss: 2.05771\n",
      "Epoch 1, Batch 60, Loss: 1.90567\n",
      "Epoch 1, Batch 65, Loss: 2.10332\n",
      "Epoch 1, Batch 70, Loss: 1.92193\n",
      "Epoch 1, Batch 75, Loss: 2.25888\n",
      "Epoch 1, Batch 80, Loss: 2.04160\n",
      "Epoch 1, Batch 85, Loss: 2.12064\n",
      "Epoch 1, Batch 90, Loss: 1.90171\n",
      "Epoch 1, Batch 95, Loss: 2.01415\n",
      "Epoch 1, Batch 100, Loss: 1.93330\n",
      "Epoch 1, Batch 105, Loss: 1.81222\n",
      "Epoch 1, Batch 110, Loss: 1.98956\n",
      "Epoch 1, Batch 115, Loss: 2.06737\n",
      "Epoch 1, Batch 120, Loss: 1.94518\n",
      "Epoch 1, Batch 125, Loss: 2.09649\n",
      "Epoch 1, Batch 130, Loss: 1.81360\n",
      "Epoch 1, Batch 135, Loss: 1.88751\n",
      "Epoch 1, Batch 140, Loss: 2.04250\n",
      "Epoch 1, Batch 145, Loss: 1.88499\n",
      "Epoch 1, Batch 150, Loss: 1.89784\n",
      "Epoch 1, Batch 155, Loss: 1.94207\n",
      "Epoch 1, Batch 160, Loss: 1.86747\n",
      "Epoch 1, Batch 165, Loss: 2.01886\n",
      "Epoch 1, Batch 170, Loss: 1.97292\n",
      "Epoch 1, Batch 175, Loss: 1.77060\n",
      "Epoch 1, Batch 180, Loss: 1.80278\n",
      "Epoch 1, Batch 185, Loss: 1.92910\n",
      "Epoch 1, Batch 190, Loss: 1.95150\n",
      "Epoch 1, Batch 195, Loss: 1.74696\n",
      "Epoch 1, Batch 200, Loss: 1.90575\n",
      "Epoch 1, Batch 205, Loss: 1.65030\n",
      "Epoch 1, Batch 210, Loss: 1.67710\n",
      "Epoch 1, Batch 215, Loss: 1.93366\n",
      "Epoch 1, Batch 220, Loss: 1.46964\n",
      "Epoch 1, Batch 225, Loss: 1.70839\n",
      "Epoch 1, Batch 230, Loss: 1.72373\n",
      "Epoch 1, Batch 235, Loss: 1.63307\n",
      "Epoch 1, Batch 240, Loss: 1.65114\n",
      "Epoch 1, Batch 245, Loss: 1.63648\n",
      "Epoch 1, Batch 250, Loss: 1.74736\n",
      "Epoch 1, Batch 255, Loss: 1.82838\n",
      "Epoch 1, Batch 260, Loss: 1.47968\n",
      "Epoch 1, Batch 265, Loss: 1.66348\n",
      "Epoch 1, Batch 270, Loss: 1.91412\n",
      "Epoch 1, Batch 275, Loss: 1.68871\n",
      "Epoch 1, Batch 280, Loss: 1.59562\n",
      "Epoch 1, Batch 285, Loss: 1.81336\n",
      "Epoch 1, Batch 290, Loss: 1.57313\n",
      "Epoch 1, Batch 295, Loss: 1.72777\n",
      "Epoch 1, Batch 300, Loss: 1.86472\n",
      "Epoch 1, Batch 305, Loss: 1.61351\n",
      "Epoch 1, Batch 310, Loss: 1.63831\n",
      "Epoch 1, Batch 315, Loss: 1.44630\n",
      "Epoch 1, Batch 320, Loss: 1.58093\n",
      "Epoch 1, Batch 325, Loss: 1.52350\n",
      "Epoch 1, Batch 330, Loss: 1.77323\n",
      "Epoch 1, Batch 335, Loss: 1.59633\n",
      "Epoch 1, Batch 340, Loss: 1.43532\n",
      "Epoch 1, Batch 345, Loss: 1.79437\n",
      "Epoch 1, Batch 350, Loss: 1.66137\n",
      "Epoch 1, Batch 355, Loss: 1.50024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[mpeg1video @ 0x37e19200] ac-tex damaged at 22 17\n",
      "[mpeg1video @ 0x37e19200] Warning MVs not available\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Batch 360, Loss: 1.60042\n",
      "Epoch 1, Batch 365, Loss: 1.54360\n",
      "Epoch 1, Batch 370, Loss: 1.73170\n",
      "Epoch 1, Batch 375, Loss: 1.52617\n",
      "Epoch 1, Batch 380, Loss: 1.56380\n",
      "Epoch 1, Batch 385, Loss: 1.59643\n",
      "Epoch 1, Batch 390, Loss: 1.89250\n",
      "Epoch 1, Batch 395, Loss: 1.59607\n",
      "Epoch 1, Batch 400, Loss: 1.57063\n",
      "Epoch 1, Batch 405, Loss: 1.38988\n",
      "Epoch 1, Batch 410, Loss: 1.51231\n",
      "Epoch 1, Batch 415, Loss: 1.56630\n",
      "Epoch 1, Batch 420, Loss: 1.64517\n",
      "Epoch 1, Batch 425, Loss: 1.41789\n",
      "Epoch 1, Batch 430, Loss: 1.62878\n",
      "Epoch 1, Batch 435, Loss: 1.47721\n",
      "Epoch 1, Batch 440, Loss: 1.70177\n",
      "Epoch 1, Batch 445, Loss: 1.35920\n",
      "Epoch 1, Batch 450, Loss: 1.79359\n",
      "Epoch 1, Batch 455, Loss: 1.52332\n",
      "Epoch 1, Batch 460, Loss: 1.29333\n",
      "Epoch 1, Batch 465, Loss: 1.41574\n",
      "Epoch 1, Batch 470, Loss: 1.52865\n",
      "Epoch 1, Batch 475, Loss: 1.48253\n",
      "Epoch 1, Batch 480, Loss: 1.35160\n",
      "Epoch 1, Batch 485, Loss: 1.31579\n",
      "Epoch 1, Batch 490, Loss: 1.34096\n",
      "Epoch 1, Batch 495, Loss: 1.27511\n",
      "Epoch 1, Batch 500, Loss: 1.41617\n",
      "Epoch 1, Batch 505, Loss: 1.51607\n",
      "Epoch 1, Batch 510, Loss: 1.35429\n",
      "Epoch 1, Batch 515, Loss: 1.48586\n",
      "Epoch 1, Batch 520, Loss: 1.45476\n",
      "Epoch 1, Batch 525, Loss: 1.40322\n",
      "Epoch 1, Batch 530, Loss: 1.26131\n",
      "Epoch 1, Batch 535, Loss: 1.42705\n",
      "Epoch 1, Batch 540, Loss: 1.39351\n",
      "Epoch 1, Batch 545, Loss: 1.30923\n",
      "Epoch 1, Batch 550, Loss: 1.26169\n",
      "Epoch 1, Batch 555, Loss: 1.41590\n",
      "Epoch 1, Batch 560, Loss: 1.18207\n",
      "Epoch 1, Batch 565, Loss: 1.51654\n",
      "Epoch 1, Batch 570, Loss: 1.44340\n",
      "Epoch 1, Batch 575, Loss: 1.24859\n",
      "Epoch 1, Batch 580, Loss: 1.11872\n",
      "Epoch 1, Batch 585, Loss: 1.53737\n",
      "Epoch 1, Batch 590, Loss: 1.32021\n",
      "Epoch 1, Batch 595, Loss: 1.50037\n",
      "Epoch 1, Batch 600, Loss: 1.50308\n",
      "Epoch 1, Batch 605, Loss: 1.17815\n",
      "Epoch 1, Batch 610, Loss: 1.18734\n",
      "Epoch 1, Batch 615, Loss: 1.37187\n",
      "Epoch 1, Batch 620, Loss: 1.55938\n",
      "Epoch 1, Batch 625, Loss: 1.38546\n",
      "Epoch 1, Batch 630, Loss: 1.20031\n",
      "Epoch 1, Batch 635, Loss: 1.46417\n",
      "Epoch 1, Batch 640, Loss: 1.19422\n",
      "Epoch 1, Batch 645, Loss: 1.73926\n",
      "Epoch 1, Batch 650, Loss: 1.31658\n",
      "Epoch 1, Batch 655, Loss: 1.64116\n",
      "Epoch 1, Batch 660, Loss: 1.13130\n",
      "Epoch 1, Batch 665, Loss: 1.09328\n",
      "Epoch 1, Batch 670, Loss: 1.30684\n",
      "Epoch 1, Batch 675, Loss: 1.29967\n",
      "Epoch 1, Batch 680, Loss: 1.11750\n",
      "Epoch 1, Batch 685, Loss: 1.26126\n",
      "Epoch 1, Batch 690, Loss: 1.14837\n",
      "Epoch 1, Batch 695, Loss: 1.09310\n",
      "Epoch 1, Batch 700, Loss: 1.11979\n",
      "Epoch 1, Batch 705, Loss: 1.26257\n",
      "Epoch 1, Batch 710, Loss: 1.48821\n",
      "Epoch 1, Batch 715, Loss: 1.20164\n",
      "Epoch 1, Batch 720, Loss: 1.12287\n",
      "Epoch 1, Batch 725, Loss: 1.02722\n",
      "Epoch 1, Batch 730, Loss: 1.25762\n",
      "Epoch 1, Batch 735, Loss: 1.16767\n",
      "Epoch 1, Batch 740, Loss: 1.13897\n",
      "Epoch 1, Batch 745, Loss: 1.23783\n",
      "Epoch 1, Batch 750, Loss: 1.49921\n",
      "Epoch 1, Batch 755, Loss: 1.63216\n",
      "Epoch 1, Batch 760, Loss: 1.27760\n",
      "Epoch 1, Batch 765, Loss: 1.69091\n",
      "Epoch 1, Batch 770, Loss: 0.94570\n",
      "Epoch 1, Batch 775, Loss: 1.32157\n",
      "Epoch 1, Batch 780, Loss: 1.28123\n",
      "Epoch 1, Batch 785, Loss: 1.54147\n",
      "Epoch 1, Batch 790, Loss: 1.07636\n",
      "Epoch 1, Batch 795, Loss: 1.32199\n",
      "Epoch 1, Batch 800, Loss: 1.14175\n",
      "Epoch 1, Batch 805, Loss: 1.40011\n",
      "Epoch 1, Batch 810, Loss: 1.06238\n",
      "Epoch 1, Batch 815, Loss: 1.18721\n",
      "Epoch 1, Batch 820, Loss: 1.40371\n",
      "Epoch 1, Batch 825, Loss: 1.45400\n",
      "Epoch 1, Batch 830, Loss: 1.19036\n",
      "Epoch 1, Batch 835, Loss: 1.21243\n",
      "Epoch 1, Batch 840, Loss: 1.47975\n",
      "Epoch 1, Batch 845, Loss: 1.21686\n",
      "Epoch 1, Batch 850, Loss: 1.34877\n",
      "Epoch 1, Batch 855, Loss: 1.33206\n",
      "Epoch 1, Batch 860, Loss: 1.40038\n",
      "Epoch 1, Batch 865, Loss: 1.54199\n",
      "Epoch 1, Batch 870, Loss: 1.36172\n",
      "Epoch 1, Batch 875, Loss: 1.67208\n",
      "Epoch 1, Batch 880, Loss: 1.26448\n",
      "Epoch 1, Batch 885, Loss: 1.25585\n",
      "Epoch 1, Batch 890, Loss: 1.25349\n",
      "Epoch 1, Batch 895, Loss: 1.13522\n",
      "Epoch 1, Batch 900, Loss: 1.38165\n",
      "Epoch 1, Batch 905, Loss: 1.09039\n",
      "Epoch 1, Batch 910, Loss: 1.39913\n",
      "Epoch 1, Batch 915, Loss: 1.36316\n",
      "Epoch 1, Batch 920, Loss: 1.16005\n",
      "Epoch 1, Batch 925, Loss: 1.51049\n",
      "Epoch 1, Batch 930, Loss: 1.33549\n",
      "Epoch 1, Batch 935, Loss: 1.34299\n",
      "Epoch 1, Batch 940, Loss: 1.27420\n",
      "Epoch 1, Batch 945, Loss: 0.99013\n",
      "Epoch 1, Batch 950, Loss: 0.92958\n",
      "Epoch 1, Batch 955, Loss: 0.92331\n",
      "Epoch 1, Batch 960, Loss: 1.04820\n",
      "Epoch 1, Batch 965, Loss: 1.07706\n",
      "Epoch 1, Batch 970, Loss: 1.51369\n",
      "Epoch 1, Batch 975, Loss: 1.18761\n",
      "Epoch 1, Batch 980, Loss: 1.17565\n",
      "Epoch 1, Batch 985, Loss: 1.22155\n",
      "Epoch 1, Batch 990, Loss: 1.45016\n",
      "Epoch 1, Batch 995, Loss: 1.37317\n",
      "Epoch 1 complete. Average loss: 1.5471, Valid batches: 995/995\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Model' object has no attribute 'state'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 76\u001b[0m\n\u001b[1;32m     72\u001b[0m    \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m complete. Average loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Valid batches: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalid_batches\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_count\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;66;03m# Save model checkpoint\u001b[39;00m\n\u001b[1;32m     75\u001b[0m    torch\u001b[38;5;241m.\u001b[39msave({\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[38;5;241m.\u001b[39mdict(),\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer_state_dict\u001b[39m\u001b[38;5;124m'\u001b[39m:optimizer\u001b[38;5;241m.\u001b[39mstate_dict(),\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m:avg_loss,\n\u001b[1;32m     79\u001b[0m     \n\u001b[1;32m     80\u001b[0m \n\u001b[1;32m     81\u001b[0m    },\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel_checkpoint_epoch_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     82\u001b[0m    clear_gpu_cache()\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/home/zeus/miniconda3/envs/cloudspace/lib/python3.10/site-packages/torch/nn/modules/module.py:1688\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1687\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1688\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Model' object has no attribute 'state'"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "        model.train()\n",
    "        print(f\"Starting epoch {epoch+1}/{epochs}\")\n",
    "        batch_count = 0\n",
    "        epoch_loss = 0\n",
    "        valid_batches = 0\n",
    "        optimizer.zero_grad()   \n",
    "        for data, labels in dataloader:\n",
    "            if data is None or len(data) == 0:\n",
    "                continue\n",
    "                \n",
    "            batch_count += 1\n",
    "            \n",
    "            try:\n",
    "                \n",
    "                data = data.permute(0, 4, 1, 2, 3).contiguous().to(device)\n",
    "                \n",
    "                #for ctc loss\n",
    "                target_lengths = torch.tensor([len(yi) for yi in labels], device=device)\n",
    "                input_lengths = torch.full((len(labels),), 75, device=device)\n",
    "                \n",
    "                # stack labels for ctc loss\n",
    "                stacked_labels = torch.cat([label.to(device) for label in labels])\n",
    "                \n",
    "               \n",
    "                log_probs = model(data)  \n",
    "                log_probs = log_probs.permute(1, 0, 2)  \n",
    "                    \n",
    "                   \n",
    "                loss = ctc_loss(log_probs, stacked_labels, input_lengths, target_lengths)\n",
    "                    \n",
    "                    \n",
    "                loss = loss / grad_accumulation_steps \n",
    "                \n",
    "                \n",
    "                if torch.isnan(loss) or torch.isinf(loss):\n",
    "                    print(f\"Batch {batch_count}: nan or inf loss detected, skipping\")\n",
    "                    # Clear any gradients from this batch\n",
    "                    optimizer.zero_grad()\n",
    "                    continue\n",
    "                \n",
    "                \n",
    "                loss.backward()\n",
    "                \n",
    "                \n",
    "                if batch_count % grad_accumulation_steps == 0:\n",
    "                \n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5) #to avoid exploding grads ( i was getting a lot of NaN and this helped)\n",
    "                    optimizer.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    clear_gpu_cache()\n",
    "                \n",
    "\n",
    "                epoch_loss += loss.item() * grad_accumulation_steps\n",
    "                valid_batches += 1\n",
    "\n",
    "                \n",
    "                if batch_count % 5 == 0:\n",
    "                    print(f\"Epoch {epoch+1}, Batch {batch_count}, Loss: {loss.item() * grad_accumulation_steps:.5f}\")\n",
    "                \n",
    "                \n",
    "                del data, log_probs, loss\n",
    "                clear_gpu_cache()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error in batch {batch_count}: {e}\")\n",
    "                clear_gpu_cache()\n",
    "             \n",
    "        if valid_batches > 0:\n",
    "           avg_loss  = epoch_loss/valid_batches\n",
    "           scheduler.step(avg_loss)\n",
    "           print(f\"Epoch {epoch+1} complete. Average loss: {avg_loss:.4f}, Valid batches: {valid_batches}/{batch_count}\")\n",
    "            \n",
    "            # Save model checkpoint\n",
    "           torch.save({\n",
    "            'model_state_dict':model.state_dict(),\n",
    "            'optimizer_state_dict':optimizer.state_dict(),\n",
    "            'loss':avg_loss,\n",
    "            \n",
    "\n",
    "           },f'model_checkpoint_epoch_{epoch+1}.pt')\n",
    "           clear_gpu_cache()\n",
    "        else:\n",
    "            print(f\"Epoch {epoch+1} had no valid batches\")\n",
    "else:\n",
    "    print(\"No valid data found\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
